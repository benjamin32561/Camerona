{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "convert_using_frozenpb.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lbi05hsDLPXs"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import tensorflow.keras.backend as K\r\n",
        "from tensorflow.python.framework import graph_io\r\n",
        "\r\n",
        "\r\n",
        "def keras_to_frozen_pb(model_in_path, \r\n",
        "                       model_out_path,\r\n",
        "                       custom_object_dict=None,\r\n",
        "                       tensor_out_name=None,\r\n",
        "                       tensorboard_dir=None):\r\n",
        "    \"\"\"\r\n",
        "    Converter that transforms keras model to frozen pb model\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        model_in_path (str): Input model path (.h5) \r\n",
        "        model_out_path (str): Output model path (dir)\r\n",
        "        tensor_out_name (str, optional): Specified name of output tensor. \r\n",
        "                                         If None, it will get default tensor name from keras model.\r\n",
        "                                         Defaults to None.\r\n",
        "        tensorboard_dir (str, optional): Output tensorboard dir path for inspecting output model graph.\r\n",
        "                                         If None, it doesn't generate. \r\n",
        "                                         Defaults to None.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    graph = tf.Graph()\r\n",
        "    with graph.as_default():\r\n",
        "        sess = tf.compat.v1.Session()\r\n",
        "        K.set_session(sess)\r\n",
        "        K.set_learning_phase(0)\r\n",
        "\r\n",
        "        # load the model to graph and sess\r\n",
        "        model = tf.keras.models.load_model(model_in_path, custom_objects=custom_object_dict)\r\n",
        "\r\n",
        "        # get the tensor_out_name \r\n",
        "        if tensor_out_name is None:\r\n",
        "            if len(model.outputs) > 1:\r\n",
        "                raise NameError(\"the model has multiple output tensor. Need to specify output tensor name.\")\r\n",
        "            else:\r\n",
        "                tensor_out_name = model.outputs[0].name.split(\":\")[0]\r\n",
        "\r\n",
        "        # freeze the graph\r\n",
        "        graphdef = tf.compat.v1.graph_util.convert_variables_to_constants(sess, graph.as_graph_def(), [tensor_out_name])\r\n",
        "        graphdef = tf.compat.v1.graph_util.remove_training_nodes(graphdef)\r\n",
        "        graph_io.write_graph(graphdef, './', model_out_path, as_text=False)\r\n",
        "\r\n",
        "\t# output tensorboard graph \r\n",
        "    if not tensorboard_dir is None:\r\n",
        "        tf.compat.v1.summary.FileWriter(logdir=tensorboard_dir, graph_def=graphdef)\r\n",
        "    \r\n",
        "    return tensor_out_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPX3hoZ6LYkH"
      },
      "source": [
        "input_keras_model = \"../input/mask.h5\"\r\n",
        "output_pb_model = \"../output/mask.pb\"\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == \"__main__\":\r\n",
        "    node_out_name = keras_to_frozen_pb(input_keras_model, output_pb_model)\r\n",
        "    print(\"the output node name is:\", node_out_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9CESWpnLdpM"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import uff\r\n",
        "import tensorrt as trt \r\n",
        "\r\n",
        "\r\n",
        "def frozen_pb_to_plan(model_path, \r\n",
        "                      output_path,\r\n",
        "                      tensor_in_name,\r\n",
        "                      tensor_out_name, \r\n",
        "                      input_size,\r\n",
        "                      data_type=trt.float32,\r\n",
        "                      max_batch_size=1,\r\n",
        "                      max_workspace=1<<30,\r\n",
        "                      tensorboard_dir=None):\r\n",
        "\r\n",
        "    # infer with pb model\r\n",
        "    graph_def = tf.GraphDef()\r\n",
        "    with tf.io.gfile.GFile(model_path, \"rb\") as f:\r\n",
        "        graph_def.ParseFromString(f.read())\r\n",
        "    \r\n",
        "    # convert TF frozen graph to uff model\r\n",
        "    uff_model = uff.from_tensorflow_frozen_model(model_path, [tensor_out_name])\r\n",
        "    \r\n",
        "    # create uff parser\r\n",
        "    parser = trt.UffParser()\r\n",
        "    parser.register_input(tensor_in_name, input_size)\r\n",
        "    parser.register_output(tensor_out_name)\r\n",
        "\r\n",
        "    # create trt logger and builder\r\n",
        "    trt_logger = trt.Logger(trt.Logger.INFO)\r\n",
        "    builder = trt.Builder(trt_logger)\r\n",
        "    builder.max_batch_size = max_batch_size\r\n",
        "    builder.max_workspace_size = max_workspace\r\n",
        "    builder.fp16_mode = (data_type == trt.float16)\r\n",
        "\r\n",
        "    # parse the uff model to trt builder\r\n",
        "    network = builder.create_network()\r\n",
        "    parser.parse_buffer(uff_model, network)\r\n",
        "\r\n",
        "    # build optimized inference engine\r\n",
        "    engine = builder.build_cuda_engine(network)\r\n",
        "\r\n",
        "    # save inference engine\r\n",
        "    with open(output_path, \"wb\") as f:\r\n",
        "        f.write(engine.serialize())\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDlX3aZvLlAx"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import tensorrt as trt\r\n",
        "import frozen_pb_to_plan\r\n",
        "\r\n",
        "BATCH_SIZE = 1\r\n",
        "H, W, C = 299, 299, 3\r\n",
        "\r\n",
        "if __name__ == \"__main__\":\r\n",
        "    '''\r\n",
        "    generate the inference engine \r\n",
        "    '''\r\n",
        "    pb_model_path = \"../output/mask.pb\"\r\n",
        "    plan_model_path = \"../output/mask_float32.plan\"\r\n",
        "    input_node_name = \"input_1\"\r\n",
        "    output_node_name = \"predictions/Softmax\"\r\n",
        "\r\n",
        "    frozen_pb_to_plan(pb_model_path,\r\n",
        "                      plan_model_path,\r\n",
        "                      input_node_name,\r\n",
        "                      output_node_name,\r\n",
        "                      [C, H, W],\r\n",
        "                      data_type=trt.float32, # change this for different TRT precision\r\n",
        "                      max_batch_size=1,\r\n",
        "                      max_workspace=1<<30)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}